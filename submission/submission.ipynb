{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mithril: Advanced Agentic FinnGen Workflow\n",
                "\n",
                "## Overview\n",
                "This notebook demonstrates the capabilities of **Mithril**, a multi-agent system built with **Google ADK** for the Kaggle Agents Intensive Capstone. It showcases:\n",
                "\n",
                "1.  **Multi-Agent Orchestration**: Planner, Researcher, Analyst, Coder, Reviewer.\n",
                "2.  **Advanced Scenarios**: Solving complex biomedical questions (GLP-1, CKD, PheWAS).\n",
                "3.  **State Management**: Long-term memory and session persistence.\n",
                "4.  **Observability**: Structured logging and tracing.\n",
                "5.  **Evaluation**: Automated assessment of agent performance.\n",
                "\n",
                "## 1. Setup & Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install google-generativeai beautifulsoup4 pandas python-dotenv matplotlib\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import pandas as pd\n",
                "from dotenv import load_dotenv\n",
                "import google.generativeai as genai\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(\"../\"))\n",
                "from src.agents.planner import PlannerAgent\n",
                "from src.memory import FileBasedMemory\n",
                "\n",
                "# Configure Environment\n",
                "load_dotenv()\n",
                "if os.getenv(\"VERTEX_API_KEY\"):\n",
                "    genai.configure(api_key=os.getenv(\"VERTEX_API_KEY\"))\n",
                "    print(\"API Key Configured\")\n",
                "else:\n",
                "    print(\"Warning: VERTEX_API_KEY not found in .env\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Advanced Research Scenarios\n",
                "\n",
                "### Scenario A: GLP-1 Agonist Weight Loss (Dynamic Code Execution)\n",
                "**Query**: \"Identify all individuals with GLP1 prescription who lost more than 20% their weight 1 year after initiation.\"\n",
                "\n",
                "*Capabilities Demonstrated*: \n",
                "- **Researcher**: Finds GLP-1 ATC codes.\n",
                "- **Coder**: Writes R code to join drug purchases with lab measurements (weight) and calculate delta.\n",
                "- **Reviewer**: Validates the logic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "planner = PlannerAgent()\n",
                "query_glp1 = \"Identify all individuals with GLP1 prescription who lost more than 20% their weight 1 year after initiation of the prescription.\"\n",
                "result_glp1 = planner.execute_workflow(query_glp1)\n",
                "print(result_glp1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scenario B: CKD Trajectories (Standard Analysis Tool)\n",
                "**Query**: \"Calculate the eGFR trajectories for patients with chronic kidney disease upon initiation of ACE inhibitors.\"\n",
                "\n",
                "*Capabilities Demonstrated*:\n",
                "- **Analyst**: Recognizes this as a BLUP analysis task.\n",
                "- **MCP Tool**: Calls `calculate_blup_slopes` via the MCP server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query_ckd = \"Calculate the eGFR trajectories for patients with chronic kidney disease upon initiation of ACE inhibitors.\"\n",
                "result_ckd = planner.execute_workflow(query_ckd)\n",
                "print(result_ckd)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scenario C: Comorbidity Overlap (Set Operations)\n",
                "**Query**: \"What is the overlap of patients diagnosed with high blood pressure and on statin with patients prescribed GLP1-RA?\"\n",
                "\n",
                "*Capabilities Demonstrated*:\n",
                "- **Complex Logic**: Intersection of 3 cohorts (Diagnosis + Drug A + Drug B)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query_overlap = \"What is the overlap of patients diagnosed with high blood pressure and on statin with patients prescribed GLP1-RA?\"\n",
                "result_overlap = planner.execute_workflow(query_overlap)\n",
                "print(result_overlap)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Session Management & Long-Term Memory\n",
                "The agent persists context across turns using `FileBasedMemory`. This allows us to inspect the state of any session."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load memory\n",
                "memory = FileBasedMemory()\n",
                "with open(\"agent_memory.json\", \"r\") as f:\n",
                "    mem_data = json.load(f)\n",
                "\n",
                "# Display the most recent session context\n",
                "last_session_id = list(mem_data[\"sessions\"].keys())[-1]\n",
                "print(f\"Session ID: {last_session_id}\")\n",
                "print(\"Context:\", json.dumps(mem_data[\"sessions\"][last_session_id][\"context\"], indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Observability: Logging & Tracing\n",
                "Every action is logged structurally. We can parse `agent_trace.log` to visualize the agent's decision path."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logs = []\n",
                "with open(\"agent_trace.log\", \"r\") as f:\n",
                "    for line in f:\n",
                "        try:\n",
                "            # Extract JSON part from log line (assuming format: TIME - NAME - LEVEL - JSON)\n",
                "            json_str = line.split(\" - \")[-1]\n",
                "            logs.append(json.loads(json_str))\n",
                "        except:\n",
                "            continue\n",
                "\n",
                "df_logs = pd.DataFrame(logs)\n",
                "print(f\"Total Actions Logged: {len(df_logs)}\")\n",
                "display(df_logs[[\"agent\", \"action\"]].head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Agent Evaluation (ADK Style)\n",
                "We can use an LLM to evaluate the quality of the agent's responses against a rubric, simulating the Google ADK evaluation suite."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_response(query, result):\n",
                "    eval_model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
                "    prompt = f\"\"\"\n",
                "    Evaluate the following agent response based on the user query.\n",
                "    \n",
                "    User Query: {query}\n",
                "    Agent Response: {result}\n",
                "    \n",
                "    Score (1-5) and explain why.\n",
                "    Criteria:\n",
                "    1. Accuracy: Did it answer the specific question?\n",
                "    2. Completeness: Did it handle all constraints?\n",
                "    3. Clarity: Is the answer easy to understand?\n",
                "    \"\"\"\n",
                "    return eval_model.generate_content(prompt).text\n",
                "\n",
                "# Evaluate the GLP-1 result\n",
                "print(evaluate_response(query_glp1, result_glp1))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}